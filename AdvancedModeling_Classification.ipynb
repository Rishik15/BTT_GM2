{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94f2ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = selected_rfe =['C8', 'C1', 'C5', 'C4', 'N7', 'C9', 'C3', 'C10', 'C2', 'N10', 'O7', 'E6', 'A10', 'age_group_45+', 'age_group_18-30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f3623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#i want to build advanced classification models using these features.The best model to use wouldl be \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed699580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, cross_val_predict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Advanced multiclass modeling cell\n",
    "# Chooses a robust tree-based multiclass model (LightGBM if available, otherwise sklearn's HistGradientBoosting)\n",
    "# Reason: gradient-boosted trees handle heterogeneous tabular features well, are robust to scaling,\n",
    "# provide feature importances, and perform strongly on multiclass problems with moderate tuning.\n",
    "\n",
    "\n",
    "# Use LightGBM if installed, otherwise fallback to sklearn's HistGradientBoostingClassifier\n",
    "try:\n",
    "    ModelClass = LGBMClassifier\n",
    "    default_params = {\n",
    "        \"objective\": \"multiclass\",\n",
    "        \"num_class\": 3,\n",
    "        \"n_jobs\": -1,\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "except Exception:\n",
    "    ModelClass = HistGradientBoostingClassifier\n",
    "    default_params = {\"random_state\": 42}\n",
    "\n",
    "# Ensure dataset 'df' exists in the notebook environment\n",
    "if 'df' not in globals():\n",
    "    raise RuntimeError(\"DataFrame 'df' not found. Please load your dataset into a variable named `df` before running this cell.\")\n",
    "\n",
    "# Determine original grit scores (0-5) column\n",
    "possible_original_cols = ['grit_original', 'grit_score', 'grit_0_5', 'original_grit', 'grit']\n",
    "original_col = next((c for c in possible_original_cols if c in df.columns), None)\n",
    "\n",
    "if original_col is None:\n",
    "    # If only a binary 'highgrit' exists, cannot reconstruct 0-5 without additional data/mapping\n",
    "    if 'highgrit' in df.columns and set(df['highgrit'].dropna().unique()).issubset({0,1}):\n",
    "        raise RuntimeError(\n",
    "            \"Only binary 'highgrit' found. To map to 3 classes you need the original 0-5 grit scores \"\n",
    "            \"or a mapping rule. Please provide the original column name or a mapping.\"\n",
    "        )\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            \"Could not find original grit column. Expected one of \"\n",
    "            f\"{possible_original_cols}. Please provide the original 0-5 grit column in `df`.\"\n",
    "        )\n",
    "\n",
    "# Map original 0-5 grit to 3 classes: 1=low, 2=medium, 3=high\n",
    "# Default mapping (adjust thresholds if you prefer different bins):\n",
    "# low: 0-1, medium: 2-3, high: 4-5\n",
    "mapping = {0:1, 1:1, 2:2, 3:2, 4:3, 5:3}\n",
    "if not set(df[original_col].dropna().unique()).issubset(set(mapping.keys())):\n",
    "    raise RuntimeError(f\"Values in {original_col} exceed expected 0-5 range. Inspect data before proceeding.\")\n",
    "\n",
    "df = df.copy()\n",
    "df['grit_class'] = df[original_col].map(mapping).astype(int)\n",
    "\n",
    "# Use the selected features defined in earlier cell (variable `columns`)\n",
    "if 'columns' not in globals():\n",
    "    raise RuntimeError(\"Variable `columns` (selected features) not found. Ensure it is defined in a previous cell.\")\n",
    "X = df[columns].copy()\n",
    "y = df['grit_class']\n",
    "\n",
    "# Basic preprocessing: simple imputation + scaling for numeric features\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = [c for c in X.columns if c not in numeric_cols]\n",
    "\n",
    "\n",
    "preprocessors = []\n",
    "if numeric_cols:\n",
    "    numeric_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    preprocessors.append((\"num\", numeric_pipeline, numeric_cols))\n",
    "if categorical_cols:\n",
    "    cat_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False))\n",
    "    ])\n",
    "    preprocessors.append((\"cat\", cat_pipeline, categorical_cols))\n",
    "\n",
    "col_transformer = ColumnTransformer(preprocessors, remainder=\"drop\")\n",
    "\n",
    "# Compute class weights to help with imbalance\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y), y=y)\n",
    "class_weight_dict = {cls: w for cls, w in zip(np.unique(y), class_weights)}\n",
    "\n",
    "# Build pipeline with model\n",
    "model_kwargs = default_params.copy()\n",
    "# LightGBM accepts 'class_weight' as dict; sklearn HGBClassifier uses 'class_weight' differently (not supported),\n",
    "# so pass class_weight only if model supports it.\n",
    "if ModelClass is LGBMClassifier:\n",
    "    model_kwargs[\"class_weight\"] = class_weight_dict\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"pre\", col_transformer),\n",
    "    (\"clf\", ModelClass(**model_kwargs))\n",
    "])\n",
    "\n",
    "# Quick hyperparameter search space (RandomizedSearchCV)\n",
    "param_dist = {\n",
    "    \"clf__n_estimators\": [100, 300, 800],\n",
    "    \"clf__learning_rate\": [0.01, 0.05, 0.1] if ModelClass is LGBMClassifier else [None],\n",
    "    \"clf__max_depth\": [3, 6, 12, -1] if ModelClass is LGBMClassifier else [3, 6, None],\n",
    "    \"clf__num_leaves\" if ModelClass is LGBMClassifier else \"clf__max_iter\": [31, 63, 127]\n",
    "}\n",
    "\n",
    "# Clean param_dist for incompatible keys with fallback model\n",
    "clean_param_dist = {}\n",
    "for k, v in param_dist.items():\n",
    "    if isinstance(k, str) and v is not None:\n",
    "        clean_param_dist[k] = v\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "search = RandomizedSearchCV(pipeline, clean_param_dist, n_iter=12, cv=cv, scoring=\"f1_macro\", n_jobs=-1, random_state=42, verbose=1)\n",
    "\n",
    "# Fit search\n",
    "search.fit(X, y)\n",
    "\n",
    "# Cross-validated predictions and evaluation\n",
    "y_pred = cross_val_predict(search.best_estimator_, X, y, cv=cv, method=\"predict\", n_jobs=-1)\n",
    "print(\"Best params:\", search.best_params_)\n",
    "print(\"\\nClassification report (cross-validated):\")\n",
    "print(classification_report(y, y_pred, digits=4))\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y, y_pred))\n",
    "\n",
    "# Feature importance (if available)\n",
    "best_clf = search.best_estimator_.named_steps['clf']\n",
    "try:\n",
    "    importances = None\n",
    "    if hasattr(best_clf, \"feature_importances_\"):\n",
    "        # get transformed feature names\n",
    "        feature_names = []\n",
    "        # numeric names\n",
    "        if numeric_cols:\n",
    "            feature_names.extend(numeric_cols)\n",
    "        # one-hot encoded categorical names\n",
    "        if categorical_cols:\n",
    "            ohe = search.best_estimator_.named_steps['pre'].transformers_[1][1].named_steps['ohe']\n",
    "            cat_feature_names = ohe.get_feature_names_out(categorical_cols).tolist()\n",
    "            feature_names.extend(cat_feature_names)\n",
    "        importances = best_clf.feature_importances_\n",
    "        feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False).head(30)\n",
    "        print(\"\\nTop feature importances:\")\n",
    "        print(feat_imp)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# End of cell.\n",
    "# NOTES / ACTIONS NEEDED FROM YOU:\n",
    "# - Confirm the mapping thresholds (0-1 -> low, 2-3 -> medium, 4-5 -> high) or provide your preferred bins.\n",
    "# - If the original 0-5 grit column has a different name, tell me the column name or supply it in `df`.\n",
    "# - Provide any additional preprocessing choices (e.g., handling specific categorical encodings)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
